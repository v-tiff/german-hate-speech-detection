{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Data augmentation.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "kbCLwxzJ46x8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0BN6T-WK8gAm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1655881450247,
     "user_tz": -120,
     "elapsed": 51593,
     "user": {
      "displayName": "Ro Ve",
      "userId": "14537681576668837982"
     }
    },
    "outputId": "bb879c3e-cade-462b-c9ca-be849ed39dd9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install wandb simpletransformers nlpaug\n",
    "\n",
    "# Handle imports\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "from simpletransformers.config.model_args import T5Args, ClassificationArgs\n",
    "from simpletransformers.t5 import T5Model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nlpaug.augmenter.word as naw\n",
    "import wandb\n",
    "\n",
    "# To show progress\n",
    "tqdm.pandas()\n",
    "\n",
    "# See the assigned GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Define constants\n",
    "BASE_PATH_PROJECT = '/content/drive/MyDrive/Colab Notebooks'\n",
    "BASE_PATH_DATA = f'{BASE_PATH_PROJECT}/data'\n",
    "TRAINING_CSV = f'{BASE_PATH_DATA}/dataset/2022_hatespeech_dataset_train.csv'\n",
    "TEST_CSV = f'{BASE_PATH_DATA}/dataset/2022_hatespeech_dataset_survey.csv'\n",
    "RT_TRANSLATED_CSV = f'{BASE_PATH_DATA}/augmented dataset/rt_translation.csv'\n",
    "CONTEXTUAL_EMBEDDING_CSV = f'{BASE_PATH_DATA}/augmented dataset/contextual_embedding.csv'\n",
    "TRAIN_CONTEXTUAL_EMBEDDING_CSV = f'{BASE_PATH_DATA}/augmented dataset/train_contextual_embedding.csv'\n",
    "TRAIN_RT_TRANSLATION_TRAINING_CSV = f'{BASE_PATH_DATA}/augmented dataset/train_rt_translation.csv'\n",
    "TRAIN_CONTEXTUAL_EMBEDDING_RT_TRANSLATION_CSV = f'{BASE_PATH_DATA}/augmented dataset/train_contextual_embedding_rt_translation.csv'\n",
    "T5_PREFIX = \"binary classification\"\n",
    "\n",
    "# Read functions\n",
    "def read_train_CSV():\n",
    "    df = pd.read_csv(TRAINING_CSV, sep=\";\",\n",
    "                     encoding=\"ISO-8859-1\",\n",
    "                     header=0,\n",
    "                     usecols=[1, 2, 3],\n",
    "                     names=[\"input_text\", \"target_text\", \"dataset\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'prefix': [T5_PREFIX for i in range(len(df))],\n",
    "        'input_text': df[\"input_text\"].str.replace('\\n', ' '),\n",
    "        'target_text': df[\"target_text\"],\n",
    "        'dataset': df[\"dataset\"].astype(str),\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_survey_CSV():\n",
    "    df = pd.read_csv(TEST_CSV, sep=\";\",\n",
    "                     usecols=[2, 3, 4], names=[\"input_text\", \"target_text\", \"dataset\"])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'prefix': [T5_PREFIX for i in range(len(df))],\n",
    "        'input_text': df[\"input_text\"].str.replace('\\n', ' '),\n",
    "        'target_text': df[\"target_text\"],\n",
    "        'dataset': df[\"dataset\"].astype(str),\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_contextual_embedding_dataset():\n",
    "    return pd.read_csv(CONTEXTUAL_EMBEDDING_CSV, sep=\",\")\n",
    "\n",
    "\n",
    "def read_rt_translation_dataset():\n",
    "    return pd.read_csv(RT_TRANSLATED_CSV, sep=\",\")\n",
    "\n",
    "\n",
    "def read_train_rt_translation_dataset():\n",
    "    return pd.read_csv(TRAIN_RT_TRANSLATION_TRAINING_CSV, sep=\",\")\n",
    "\n",
    "\n",
    "def read_train_contextual_embedding_dataset():\n",
    "    return pd.read_csv(TRAIN_CONTEXTUAL_EMBEDDING_CSV, sep=\",\")\n",
    "\n",
    "\n",
    "def read_train_contextual_embedding_rt_translation_dataset():\n",
    "    return pd.read_csv(TRAIN_CONTEXTUAL_EMBEDDING_RT_TRANSLATION_CSV, sep=\",\")\n",
    "\n",
    "\n",
    "# Log metrics\n",
    "def log_test_metrics(y_true, y_pred, metric_prefix):\n",
    "    assert not None in y_true, \"None in y_true\"\n",
    "    assert not None in y_pred, \"None in y_pred\"\n",
    "    assert len(y_pred) == len(y_true), \"Unequal length of y_pred and y_true\"\n",
    "    y_true = [int(i) for i in y_true]\n",
    "    y_pred = [int(i) for i in y_pred]\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    wandb.log({f\"{metric_prefix}_TP\": tp})\n",
    "    wandb.log({f\"{metric_prefix}_TN\": tn})\n",
    "    wandb.log({f\"{metric_prefix}_FP\": fp})\n",
    "    wandb.log({f\"{metric_prefix}_FN\": fn})\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred)\n",
    "    wandb.log({f\"{metric_prefix}_accuracy\": accuracy})\n",
    "    wandb.log({f\"{metric_prefix}_f1-score\": f1_score})\n",
    "\n",
    "# Load datasets\n",
    "def print_dataset_statistics(df, dataset_str, true_label_col=\"target_text\", is_t5=False):\n",
    "    print(f'Number of entries in {dataset_str}: {len(df)}')\n",
    "\n",
    "    if is_t5:\n",
    "        print(f'Number of 0s in {dataset_str}: {len(df[df[true_label_col] == \"0\"])}')\n",
    "        print(f'Number of 1s in {dataset_str}: {len(df[df[true_label_col] == \"1\"])}')\n",
    "    else:\n",
    "        print(f'Number of 0s in {dataset_str}: {len(df[df[true_label_col] == 0])}')\n",
    "        print(f'Number of 1s in {dataset_str}: {len(df[df[true_label_col] == 1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Contextual word embedding"
   ],
   "metadata": {
    "id": "0WcgUrtU93zW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = read_train_CSV().head()\n",
    "train_df, _ = train_test_split(train_df, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='deepset/gbert-base',\n",
    "    action=\"substitute\",\n",
    "    device='cpu'\n",
    ")\n",
    "\n",
    "contextual_embedding_df = train_df.copy()\n",
    "contextual_embedding_df['input_text'] = train_df['input_text'].progress_apply(aug.augment)\n",
    "\n",
    "contextual_embedding_df.to_csv(CONTEXTUAL_EMBEDDING_CSV, index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_eqw5Zz93Jh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1655850022171,
     "user_tz": -120,
     "elapsed": 3607788,
     "user": {
      "displayName": "Karl Lachhaft",
      "userId": "03692334522368575595"
     }
    },
    "outputId": "5bda2dd6-329f-43e5-e9ed-5ffd09948010",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RT translation"
   ],
   "metadata": {
    "id": "_k8Tj3W2AO2R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = read_train_CSV()\n",
    "train_df, _ = train_test_split(train_df, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "aug = naw.BackTranslationAug(\n",
    "    from_model_name='Helsinki-NLP/opus-mt-de-en',\n",
    "    to_model_name='Helsinki-NLP/opus-mt-en-de',\n",
    "    device='cpu'\n",
    ")\n",
    "rt_translation = train_df.copy()\n",
    "rt_translation['input_text'] = train_df['input_text'].progress_apply(aug.augment)\n",
    "\n",
    "rt_translation.to_csv(RT_TRANSLATED_CSV, index=False)\n"
   ],
   "metadata": {
    "id": "mieozDy1Ao-E",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1655897881197,
     "user_tz": -120,
     "elapsed": 7,
     "user": {
      "displayName": "Ro Ve",
      "userId": "14537681576668837982"
     }
    },
    "outputId": "bbf0a155-379e-43c7-8c92-38b8ce6ccc18",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create combined datasets"
   ],
   "metadata": {
    "id": "BcKM15IcOXiO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = read_train_CSV()\n",
    "train_df, _ = train_test_split(train_df, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "contextual_embedding_df = read_contextual_embedding_dataset()\n",
    "rt_translation_df = read_rt_translation_dataset()\n",
    "\n",
    "train_contextual_embedding_df = pd.concat([train_df, contextual_embedding_df])\n",
    "train_rt_translation_df = pd.concat([train_df, rt_translation_df])\n",
    "train_contextual_embedding_rt_translation_df = pd.concat([train_df, contextual_embedding_df, rt_translation_df])\n",
    "\n",
    "\n",
    "train_contextual_embedding_df.to_csv(TRAIN_CONTEXTUAL_EMBEDDING_CSV, index=False)\n",
    "train_rt_translation_df.to_csv(TRAIN_RT_TRANSLATION_TRAINING_CSV, index=False)\n",
    "train_contextual_embedding_rt_translation_df.to_csv(TRAIN_CONTEXTUAL_EMBEDDING_RT_TRANSLATION_CSV, index=False)\n"
   ],
   "metadata": {
    "id": "yZhzlfZJOZ_-"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}